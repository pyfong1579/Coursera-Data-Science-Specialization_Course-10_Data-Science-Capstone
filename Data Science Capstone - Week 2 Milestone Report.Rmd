---
title: "Data Science Capstone - Week 2 Milestone Report"
author: "P.Y.Fong"
date: "`r format(Sys.Date(),'%d %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(cache= T,
               echo= T,
               message= F,
               warning= F,
               comment= "")
```

##  1.0 Executive Summary.
The Key partners for this project are Swiftkey and Coursera.  
The project explores the Natural Language Processing facet of Data Science where a large text corpus of documents will be used to predict the next word on a preceding input.  
This goal of this project milestone report is to display exploratory insights obtained while working with the data provided - [Capstone Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip), and that the project is on track to create a prediction algorithm.  
This, report published on [R Pubs](http://rpubs.com/) explains the exploratory analysis and goals for the eventual app / algorithm.  
This document intends to explain only the major features of the data that have been identified and a brief summary of the plans for creating the prediction algorithm and Shiny app.  

####  1.1 Major findings.
*Restricting exploration to the enUS corpus:*

1.  The downloaded data is large and required manipulation, cleaning and sorting. Several functions were created in this report for that purpose.
2.  More data wrangling may be needed.
3.  The summary statistics in this report show the direction to take for the predictive model in particular the use of n-grams as training data.
4.  Some interesting observations were noted including the small size of the bi-gram.
5.  Fairly large use of computing resources on just the sample set.

In conclusion, suggestions and comments are welcome from the peer review to be conducted.

##  2.0 Libraries.
```{r Libraries, cache= F}
library(stringi)    # For string processing and examination.
library(tm)         # for text mining.
library(SnowballC)  # stem words.
library(RWeka)      # interface to Java machine learning tools for data mining.
library(RColorBrewer) # color palette.
library(ggwordcloud)# word frequency visualizer.
library(ggplot2)    # Grammar of Graphics package for Visualizations.
library(grid)       # To manipulate visualizations.
library(gridExtra)  # supplement to grid.
library(kableExtra) # To display tables.

```

##  3.0 Data Corpus.
### 3.1 Data Load.
Only the en_US files were used for exploratory work and were downloaded and stored locally for convenience.

*   blogs: ./final/en_US.blogs.txt
*   news: ./final/en_US.news.txt
*   twitter: ./final/en_US.twitter.txt

```{r Data Read}
# create paths
blogs_p <- "./final/en_US/en_US.blogs.txt"
news_p <- "./final/en_US/en_US.news.txt"
twitter_p <- "./final/en_US/en_US.twitter.txt"

# read data
blogs <- readLines(blogs_p, encoding= "UTF-8", skipNul=T)
news <- readLines(news_p, encoding= "UTF-8", skipNul=T)
twitter <- readLines(twitter_p, encoding= "UTF-8", skipNul=T)
```

### 3.2 Data Summary.
The data sets were examined and tabulated to give a sense of the data.  
The intent here was to:

1.  Assess the object sizes.
2.  Assess the available number of lines, characters, white space characters and words.
3.  Calculate some stats on the number of words per line (WPL).

```{r Summary Function}
# Start function
sumtab <- function(t1, t2, t3) {
    # capturing row labels
    lbl1 <- substitute(t1); lbl2 <- substitute(t2); lbl3 <- substitute(t3)
    lbl <- sapply(c(lbl1, lbl2,lbl3),deparse)
    # getting object sizes
    SizeMB <- c(object.size(t1), object.size(t2), object.size(t3))/1024^2
    # calculating Words per Line Stats
    WPL= sapply(list(t1, t2, t3), function(x) summary(stri_count_words(x))[c('Min.', 'Mean', 'Max.')])
    rownames(WPL)=c('WPLmin', 'WPLmean', 'WPLmax')
# setting up output table
stats= data.frame(Dataset= lbl, SizeMB,
                  t(rbind(sapply(list(t1,t2,t3),stri_stats_general)[c('Lines','Chars'),],
                          sapply(list(t1,t2,t3),stri_stats_latex)[c('CharsWhite','Words'),], WPL)))
# Printing summary table
kbl(stats, digits=2, format.args = list(big.mark= ",", scientific = F)) %>%
      kable_minimal(full_width = T, position = "float_right")
} # End Function

# Summary of raw datasets
sumtab(blogs, news, twitter)
```
The initial examination showed that the data objects were very large.  
Surprisingly, the twitter data was the largest but held less words than blogs.  
The statistics also show the interesting issue of words per line ranging from 0 to over 6,700 across all data sets.  
The number of white space characters was also significant at around 20 - 30% of the datasets.

### 3.3 Data Sampling.

Given the size of the data, further exploratory work was carried out on smaller samples (1%) to reduce computing resources requirements.

```{r Data Sample}
# Sampling
set.seed(1234) # Repeatable sampling.
blogs_smp <- sample(blogs, length(blogs) * 0.01)
news_smp <- sample(news, length(news) * 0.01)
twitter_smp <- sample(twitter, length(twitter) * 0.01)

#Summary of samples
sumtab(blogs_smp, news_smp, twitter_smp)
```
As can be seen in the table above the 1% samples were easier to work with.  
However, note that the means of the words per line in the samples do not change much from the original datasets.

### 3.4 Data Cleaning and Corpus consolidation.
The sample data sets were cleaned using the tm package with the following steps before being consolidated into a single corpus:

1.  Convert characters to lowercase.
2.  Remove numbers.
3.  Remove stop words. refer: stopwords() vector from tm package.
4.  Remove punctuation.
5.  Removing profanity using list from [www.cs.cmu.edu](http://www.cs.cmu.edu/~biglou/resources/bad-words.txt).
6.  Strip extra white spaces.

```{r Corpus}
# Set up vector of profanity words
profanity <- file("./final/bad-words.txt", "r")
profanV <- VectorSource(readLines(profanity))

# Cleaning function
clean <- function(obj) {
obj_clean <- VCorpus(VectorSource(obj))
obj_clean <- tm_map(obj_clean, tolower)
obj_clean <- tm_map(obj_clean, removeNumbers)
obj_clean <- tm_map(obj_clean, removeWords, stopwords('SMART'))
obj_clean <- tm_map(obj_clean, removePunctuation, ucp= T)
obj_clean <- tm_map(obj_clean, removeWords, profanV)
obj_clean <- tm_map(obj_clean, stripWhitespace)
obj_clean <- tm_map(obj_clean, PlainTextDocument) # correcting data type after cleaning
}
# Display  summary table
blogs_cln <- clean(blogs_smp)
blogs_c_txt <- unlist(sapply(blogs_cln, '[', "content"))
news_cln <- clean(news_smp)
news_c_txt <- unlist(sapply(news_cln, '[', "content"))
twitter_cln <- clean(twitter_smp)
twitter_c_txt <- unlist(sapply(twitter_cln, '[', "content"))
sumtab(blogs_c_txt, news_c_txt, twitter_c_txt)

# Consolidating datasets
enUS_corp <- c(blogs_cln, news_cln, twitter_cln)
```
The observation here, is that the cleaned data objects are much smaller after cleaning.

### 3.5 Tokenizations and Visualizations.
The corpus was then tokenized into N-grams using the RWeka package.  
Visualization was created using tables, wordcloud diagrams and bar plots.  

The functions that were used as follows:
```{r Token Display}
# tokenization function
tokenize <- function(data, ngram, minfreq) {
    token <- function(data) NGramTokenizer(data, Weka_control(min= ngram, max= ngram))
    matrix <- TermDocumentMatrix(data, control= list(tokenize= token))
    freq <- findFreqTerms(matrix, lowfreq= minfreq)
    freqdata <- sort(rowSums(as.matrix(matrix[freq,])), decreasing= T)
    freqdata <- data.frame(Phrase= names(freqdata), Frequency= freqdata, row.names = NULL)
    return(freqdata)
}

# display function
display <-function (ngramset,title) {
    # Create Table
    table <- tableGrob(ngramset[1:10,], rows = NULL)
    # Create Wordcloud
    wc <- ggplot(ngramset[1:10,], aes(label= Phrase, size= Frequency, color= Frequency)) +
      geom_text_wordcloud() +
      scale_size_area(max_size= 12) +
      scale_color_distiller(palette = "YlOrRd", direction= 1) 
    # Create BarPlot
    plot <- ggplot(ngramset[1:10,], aes(y = reorder(Phrase, Frequency), x = Frequency,
                                        fill = Frequency)) +
      geom_bar(stat = "identity") +
      scale_fill_distiller(palette = "YlOrRd", direction= 1) +
      theme(legend.position= "none") +
      labs(y="Phrase")
    # Create Coverage Plot
    p <- cumsum(ngramset$Frequency)/sum(ngramset$Frequency)
    p.5 <- which(p>+0.5)[1]; p.9 <- (which(p>=.9)[1])
    p <- data.frame(p)
    p$idx <- as.numeric(row.names(p))
    names(p) <- c("pr","idx")
    cover <- ggplot(p, aes(y= idx, x= pr, fill= pr)) +
        geom_col() +
        scale_fill_distiller(palette = "YlOrRd", direction= 1) +
        theme(legend.position= "none") +
        labs(y= "Phrase Count", x= "Probability of Coverage") +
        geom_vline(xintercept= 0.5, color= "steelblue", size = 1) +
        geom_vline(xintercept= 0.9, color= "steelblue", size = 1) +
        annotate("text", x = .45, y = 1025, label = paste(p.5, "Phrases", "\n", "@ 50% Coverage")) +
        annotate("text", x = .85, y = 1025, label = paste(p.9, "Phrases", "\n", "@ 90% Coverage"))
    # Assemble Figure
    layout <- rbind(c(1,2),
                    c(3,4))
    grid.arrange(table, cover, plot, wc,
                 layout_matrix =layout,
                 top= textGrob(paste("Top 10", title, "out of",
                                     as.character(dim(ngramset)[1]), sep= " "),
                 gp= gpar(fontsize= 20)))
}
```

####    3.5.1 Uni-Grams.
Uni-gram tokenization was limited to those words with frequency > 50
```{r unigrams, fig.height= 10, fig.width= 10}
unigram <- tokenize(enUS_corp, 1, 50)
display(unigram, "Uni-grams")
```

####    3.5.2 Bi-Grams.
Bi-gram tokenization was limited to those words with frequency > 20
```{r bigrams, fig.height= 10, fig.width= 10}
bigram <- tokenize(enUS_corp, 2, 20)
display(bigram, "Bi-grams")
```

####    3.5.3 Tri-Grams.
Tri-gram tokenization was limited to those words with frequency > 2
```{r trigrams, fig.height= 10, fig.width= 10}
trigram <- tokenize(enUS_corp, 3, 2)
display(trigram, "Tri-grams")
```

###   4.0 Analysis.
From section 3.5 it was noted that:

*   the tokenized n-gram dictionaries were representative of the total enUS corpus at the 90% coverage level with around 800 phrases. Whether this is representative of the language is difficult to determine at this stage.
*   the n-grams show a large number of phrases have a high frequency of occurrence.
*   however, bigram phrase count was quite small which was unexpected.
*   cleaning the data with the listed steps significantly reduces the data set size.
*   foreign language phrases were not considered yet and may involve a foreign language filter dictionary similar to the profanity filter used.
*   some phrases are repeated such as the "happy mothers day" phrase in the trigrams suggesting further cleaning using stemming (root words conversion) may be necessary.
*   the functions used above should be transferable to the actual data sets provided some performance tuning is done.
```{r memory performance}
gc()
```

###   5.0 Plans for Prediction Model.
The next step in the project is text prediction modeling.  
The following studies will need to be conducted:

*   N-Gram modeling of the full text data sets. (May even require Quad grams)
*   model optimization especially with regards to memory utilization.
*   implement the model as a Shiny App.
*   the model design will have to consider invalid and non matching inputs. 